{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import *  # for figsize\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import shapiro\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "import sklearn as sl\n",
    "import sklearn.linear_model\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from future_encoders import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit as SSS\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set the decimal argument, when commatas are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = pd.read_csv(\"existing.csv\", sep = \";\", decimal = \",\") \n",
    "new = pd.read_csv(\"new.csv\", sep = \";\", decimal = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [existing, new]\n",
    "whole = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(whole3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#existing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best_seller_rank has 15 missing or incorrect values, Width has one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary = whole4.describe()\n",
    "#summary = summary.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary #whole4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are not all attributes displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert object columns to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataframe without Product_type to do conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2 = whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2 = whole2.drop(columns=[\"Product_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = whole2.select_dtypes(exclude=['int64']).columns\n",
    "\n",
    "whole2[cols] = whole2[cols].apply(pd.to_numeric, downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Product_type column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2[\"Product_type\"] = whole[\"Product_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole2[whole2.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole2[whole2.isnull().any(axis=1)][\"Best_seller_rank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Best_seller_rank has 15 NANs and Width has one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NANs with median in Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an imputer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2[\"Width\"].fillna(whole2[\"Width\"].median(),inplace=True);\n",
    "whole2[\"Width\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2[\"Width\"][97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete rows which have NAN in Best_seller_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole2 = whole2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the summary of whole2 again, Best seller rank has outliers, and since it is probably highly correlated with Volume, we can safely ignore it and drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole3 = whole2.drop(columns=[\"Best_seller_rank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter-plot matrix & SHOW-ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [\"5Stars\",'4Stars',\n",
    "# '3Stars',\n",
    "# '2Stars',\n",
    "# '1Stars',\n",
    "#       ]#'Positive_service_review',\n",
    "# #'Negative_service_review',\n",
    "# #'Would_consumer_recomend__product',]\n",
    "#\n",
    "#sns.pairplot(whole3[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"5Stars\",\"Positive_service_review\", \"Volume\"]\n",
    "#scatter_matrix(training1[attributes], figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training3.plot(kind=\"scatter\", x=\"5Stars\", y=\"Volume\",\n",
    "             alpha=0.5, figsize=(20,10))\n",
    "plt.axis([0,1000,0,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training1.plot(kind=\"scatter\", x=\"Positive_service_review\", y=\"Volume\",\n",
    "#             alpha=0.5, figsize=(20,10))\n",
    "#plt.axis([0,400#,0,4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = [10,30,45,60,75,90,105,500]\n",
    "sns.distplot(pred_set[\"5Stars\"], bins = bins1) #use prediction set and training set (1.o) to compare\n",
    "plt.xlim(0,110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the correlations of the whole dataset (training and test set included), which is a \"quick-and-dirty-solution\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix star-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[\"5Stars\",'4Stars',\n",
    "'3Stars',\n",
    "'2Stars',\n",
    "'1Stars', \"Volume\"]\n",
    "#whole3[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just keep 5-stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix 5-stars, negative-, positive-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Positive_service_review',\n",
    " 'Negative_service_review',\"5Stars\", \"Volume\"]\n",
    "\n",
    "#whole3[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to keep positive reviews, it is correlated with volume, well enough, and not too correlated with 5-stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Would_consumer_recomend__product',\n",
    " 'Weigth',\n",
    " 'Depth',\n",
    " 'Width',\n",
    " 'Heigth',\n",
    " 'Profit_margin',\"5Stars\",'Positive_service_review', \"Volume\"]\n",
    "\n",
    "#whole3[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly included attributes are not strongly correlated with the Volume. We won't keep them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Profit_margin',\n",
    " 'Volume',\n",
    " 'Relative_Price',\n",
    " 'Environment_Impact',\n",
    " 'Durability_standard',\n",
    " 'Product_type',\"5Stars\",'Positive_service_review']\n",
    "\n",
    "#whole3[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the added attributes is highly enough correlated with the Volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first approach of selecting a combination of attributes, we focused on the correlation matrices, i.e. we looked for linear relationsships. Thus we will, as a first approximation, try to get good prediction results by applying a linear model.\n",
    "We chose 5-stars and positive service reviews as only relevent attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeps = [\"5Stars\",'Positive_service_review',\"Volume\"]\n",
    "whole4 = whole3[keeps]\n",
    "#whole4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations before only targeted rows/columns of the training set (1.o.), now we have to remove outliers (before normalizing later). The removal should naturally only be applied to the training set (2.o.). \n",
    "Therefore we have to split into training set (1.o.) and prediction set and then again the training set (1.o.) into training set (2.o.) and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training-set (1 order) and prediction-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find index, where prediction set begins by using summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whole4.iloc[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#whole6.index[230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set = whole4.iloc[range(230,len(whole4),1)]\n",
    "#pred_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1 = whole4.iloc[range(0,229,1)]\n",
    "#training1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1.reset_index(drop=True, inplace= True)\n",
    "#training1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to get a graphical view of the chosen attributes and the dependent variable. For this we go back to the  scatter-plot show zone, and display a scatter plot matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationsship between the positive service reviews does not seem linear, interestingly, because the correlation was not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From it we see that the relationsship between 5Stars and volume is clearly linear only after a certain amount of 5-star-reviews is reached. \n",
    "To determine this value, we take a look at the histogram (in the show-zone)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we conclude that we set the minimum limit of 5-star reviews, which qualifies examples for our predictive model, to 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training2 = training1.loc[training1[\"5Stars\"] > 10]\n",
    "#training2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set2 = pred_set.loc[pred_set[\"5Stars\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_set3 = pred_set2.loc[pred_set2[\"5Stars\"] < 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set2.reset_index(drop=True, inplace=True)\n",
    "pred_set2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set3 = pred_set2.drop(columns=[\"Positive_service_review\", \"Volume\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at histograms for Str.Spl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the show-zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin attributes for Str.Spl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training2['5Stars_cat'] = np.ceil(training2['5Stars'].values / 80)\n",
    "training2['5Stars_cat'].where(training2['5Stars_cat'] < 5, 5.0, inplace=True)\n",
    "training2.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training2[\"5Stars_cat\"].value_counts()\n",
    "#training2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers & drop NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training3 = training2[np.abs(training2-training2.mean()) <= (3*training2.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set[\"Positive_service_review\"].sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole4[\"Positive_service_review\"].sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholeD[\"Volume\"].sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example with index 49 is the maximum value of all 3 attributes. We will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training3 = training2[training2.Volume < training2.Volume.quantile(.95)]\n",
    "training3 = training3.dropna()\n",
    "training3.reset_index(drop=True, inplace= True)\n",
    "#wholeB = wholeA[wholeA[\"5Stars\"] < wholeA[\"5Stars\"].quantile(.95)]\n",
    "#wholeD = wholeB[wholeB[\"Positive_service_review\"] < wholeB[\"Positive_service_review\"].quantile(.95)]\n",
    "#training3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training3.columns = [\"5_stars\", \"positive_service_review\", \"volume\", \"5_stars_cats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps = [\"5Stars\", \"Positive_service_review\", \"Volume\"] # do not normalize 5stars_cat and ????dependent variable????\n",
    "#training4 = training3[keeps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_set2.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = training4.values #returns a numpy array\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#x_scaled = min_max_scaler.fit_transform(x)\n",
    "#training5 = pd.DataFrame(x_scaled)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop 5Stars_cat & Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training6 = pd.concat([training5, training3[\"5Stars_cat\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training set of first order is really quite small (229 examples), we have to think about making the size of the test set not too small. So we will use a 60/40 split. \n",
    "In addition this small training size means stratifying is important.\n",
    "We should consider stratifying with respect to the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to look at the distribution of the two independent variables to discretize it into stratas for the stratified sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ssplit = SSS(n_splits=1, test_size=0.4, random_state=42)\n",
    "Ssplit\n",
    "for train_index, test_index in Ssplit.split(training3,training3[\"5_stars_cats\"]):\n",
    "    train_set = training3.loc[train_index]\n",
    "    test_set = training3.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset index of train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=2, shuffle=False, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train_index, test_index in skf.split(training2,training2[\"5Stars_cat\"]):\n",
    "#    train_set = training2.loc[train_index]\n",
    "#    test_set = training2.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st_sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole5 = st_sc.fit_transform(whole4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole6 = pd.DataFrame(whole5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified - Cross Validation & Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=False, random_state=26) #48 /3 = 16 (number of obs per fold), \n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cross-validation-training and testing (for each fold there was a seperate test) we first used positive_service_reviews in addition to 5_stars, but the results, including solely 5Stars, were slightly better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {}\n",
    "i = 0\n",
    "for train_indices, test_indices in skf.split(train_set[[\"5_stars\"]],train_set[\"5_stars_cats\"]):\n",
    "    i = i + 1\n",
    "    \n",
    "    #save the indices in dict to apply different models to test set \n",
    "    \n",
    "    indices[i]=train_indices   \n",
    "    lin_reg.fit(train_set.loc[train_indices, [\"5_stars\"]],\n",
    "                train_set.loc[train_indices, [\"volume\"]])\n",
    "    print(\"\\n\")\n",
    "    print(str(i) +\". fold\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # printing predictions on validation sets (not needed)\n",
    "    \n",
    "    #print(lin_reg.predict(train_set3.loc[test_indices, [\"5_stars\", \"positive_service_review\"]]))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #printing R-squared on validation set\n",
    "    print(\"R-squared on validation set\")\n",
    "    print(lin_reg.score(train_set.loc[test_indices,[\"5_stars\"]],train_set.loc[test_indices,[\"volume\"]]))\n",
    "    \n",
    "    #printing coefficients of regressions on different folds (not needed)\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "    #print(\"coefficients\")\n",
    "    #print(lin_reg.coef_)\n",
    "    #print(\"\\n\")\n",
    "    #print(\"intercept\")\n",
    "    #print(lin_reg.intercept_)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    # prinint R-sq. of regression models of the different folds with help of indices dictionary\n",
    "    print(\"\\n\")\n",
    "    print(\"R-squared on test set\")\n",
    "    print(lin_reg.fit(train_set.loc[indices[i], [\"5_stars\"]],\n",
    "                                      train_set.loc[indices[i],[\"volume\"]]).score(test_set[[\"5_stars\"]],\n",
    "                                                                                   test_set[\"volume\"]))\n",
    "    print(\"\\n\")\n",
    "    print(\"RMSE on test set\")\n",
    "    print(sqrt(mean_squared_error(test_set[\"volume\"], lin_reg.predict(test_set[[\"5_stars\"]]))))\n",
    "    \n",
    "    #the extra brackets around  5Stars in the line above are crucial, since more than one attr. is expected\n",
    "    \n",
    "    #save predictions of prediction set for each fold in an extra column of the pred set3 dataframe\n",
    "    pred_set3[str(i) +\". fold predictions\"] = lin_reg.predict(pred_set3[[\"5Stars\"]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are constructing ranges of predictions consisting of the predictions of the first fold and the second fold, since they have the best combined R-squared values and their predictions are closest together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the distributions of 5-stars in the training set (1.o.) and the prediction set shows, that the one of the prediction set has a lot more values bigger than 90. This means we can only successfully predict volumes of products whose number of 5-stars varies from 10 until 90. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction set is accordingly adapted in section 4.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_set3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop & reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set4 = pred_set3.drop(columns = [\"3. fold predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pred_set4.columns.tolist()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = [cols[0], cols[2],cols[1]]\n",
    "cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set5 = pred_set4[cols2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_set5.columns =[\"5_stars\", \"lower_prediction_limit\", \"upper_prediction_limit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the additional column by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 5Stars as index for new and pred_set3\n",
    "\n",
    "new2 = new.set_index(\"5Stars\")\n",
    "pred_set6 = pred_set5.set_index(\"5_stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([new2, pred_set6], axis=1, sort=False, join_axes=[new2.index]) #join_axes to keep index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data in excel file (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('predictions.xlsx')\n",
    "predictions2.to_excel(writer,'predictions')\n",
    "#df2.to_excel(writer,'Sheet2')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
